<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    <link rel="canonical" href="http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/">
    
    
    <title>Coursera ML(4)-Logistic Regression | 李飞阳 | PM、Coder、Data mining</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#0288D1">
    
    
    <meta name="keywords" content="Machine Learning,读书笔记,Python">
    <meta name="description" content="本节笔记对应第三周Coursera课程  binary classification problem">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera ML(4)-Logistic Regression">
<meta property="og:url" content="http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/index.html">
<meta property="og:site_name" content="李飞阳">
<meta property="og:description" content="本节笔记对应第三周Coursera课程  binary classification problem">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170327/112711489.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170327/182954425.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170327/183239547.png">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170327/184128038.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:updated_time" content="2017-03-28T07:49:36.625Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera ML(4)-Logistic Regression">
<meta name="twitter:description" content="本节笔记对应第三周Coursera课程  binary classification problem">
<meta name="twitter:image" content="http://cdn.mmmxcc.cn/blog/20170327/112711489.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
    
        <link rel="alternative" href="/atom.xml" title="李飞阳" type="application/atom+xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.4.11">
    <script>window.lazyScripts=[]</script>
</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/logo.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Wing Lee</h5>
          <a href="mailto:mmmwhy@qq.com" title="mmmwhy@qq.com" class="mail">mmmwhy@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/Coursera-ML"  >
                <i class="icon icon-lg icon-paperclip"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/mmmwhy" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/2016/01/18/resume/"  >
                <i class="icon icon-lg icon-rocket"></i>
                关于
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/link.html"  >
                <i class="icon icon-lg icon-link"></i>
                链接
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://ss.feiyang.li/auth/login"  >
                <i class="icon icon-lg icon-bolt"></i>
                肥羊免费SS
              </a>
            </li>
        
      </ul>
    </div>

    <div id="myTime">
 <object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=8,0,0,0" width="180" height="80" id="honehoneclock" align="middle">
 <param name="allowScriptAccess" value="always">
 <param name="movie" value="http://chabudai.sakura.ne.jp/blogparts/honehoneclock/honehone_clock_wh.swf">
 <param name="quality" value="high">
 <param name="bgcolor" value="#ffffff">
 <param name="wmode" value="transparent">
 <embed wmode="transparent" src="http://chabudai.sakura.ne.jp/blogparts/honehoneclock/honehone_clock_wh.swf" quality="high" bgcolor="#ffffff" width="180" height="80" name="honehoneclock" align="middle" allowscriptaccess="always" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer">
 </object>
 </div>



  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Coursera ML(4)-Logistic Regression</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Coursera ML(4)-Logistic Regression</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-03-27T11:13:38.000Z" itemprop="datePublished" class="page-time">
  2017-03-27
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Coursera-ML/">Coursera ML</a></li></ul>

            
        </h5>
    </div>

    

</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Classification-and-Representation"><span class="post-toc-number">1.</span> <span class="post-toc-text">Classification and Representation</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Hypothesis-Representation"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Hypothesis Representation</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Decision-Boundary"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Decision Boundary</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Logistic-Regression-Model"><span class="post-toc-number">2.</span> <span class="post-toc-text">Logistic Regression Model</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Cost-function-for-one-variable-hypothesis"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Cost function for one variable hypothesis</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Simplified-Cost-Function-and-Gradient-Descent"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Simplified Cost Function and Gradient Descent</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Gradient-Descent"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Gradient Descent</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Multiclass-Classification-One-vs-all"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Multiclass Classification: One-vs-all</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Solving-the-Problem-of-Overfitting"><span class="post-toc-number">3.</span> <span class="post-toc-text">Solving the Problem of Overfitting</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#The-Problem-of-Overfitting"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">The Problem of Overfitting</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#address-the-issue-of-overfitting"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">address the issue of overfitting</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Cost-Function"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Cost Function</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Regularized-Linear-Regression"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Regularized Linear Regression</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Summary"><span class="post-toc-number">4.</span> <span class="post-toc-text">Summary</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Logistic-Regression-Model-1"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">Logistic Regression Model</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#h-theta-x-是假设函数"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">$h_\theta(x)$是假设函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Cost-Function-1"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">Cost Function</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Gradient-Descent-1"><span class="post-toc-number">4.1.3.</span> <span class="post-toc-text">Gradient Descent</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Solving-the-Problem-of-Overfitting-1"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">Solving the Problem of Overfitting</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Cost-Function-2"><span class="post-toc-number">4.2.1.</span> <span class="post-toc-text">Cost Function</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Gradient-Descent-2"><span class="post-toc-number">4.2.2.</span> <span class="post-toc-text">Gradient Descent</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-Coursera-ML-4-Logistic-Regression"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Coursera ML(4)-Logistic Regression</h1>
        <div class="post-meta">
            <time class="post-time" title="2017年03月27日 19:13" datetime="2017-03-27T11:13:38.000Z"  itemprop="datePublished">2017-03-27</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Coursera-ML/">Coursera ML</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <script src="/assets/js/APlayer.min.js"> </script><script src="/assets/js/DPlayer.min.js"> </script><blockquote>
<p>本节笔记对应<a href="https://www.coursera.org/learn/machine-learning/home/week/3" target="_blank" rel="external">第三周Coursera课程</a>  <strong>binary classification problem</strong></p>
</blockquote>
<hr>
<a id="more"></a>
<p><strong>Classification is not actually a linear function.</strong></p>
<h1 id="Classification-and-Representation"><a href="#Classification-and-Representation" class="headerlink" title="Classification and Representation"></a>Classification and Representation</h1><h2 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h2><ul>
<li>Sigmoid Function(or we called Logistic Function) $$\begin{align*}& h_\theta (x) = g ( \theta^T x ) \newline \newline& z = \theta^T x \newline& g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}$$ 
Sigmoid Function 可以使输出值范围在$(0,1)$之间。$g(z)$对应的图为：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170327/112711489.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<ul>
<li>$h_\theta(x)$ will give us the <strong>probability</strong> that our output is 1.</li>
<li>Some basic knowledge of discrete $$\begin{align*}& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline& P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}$$ 
</li>
</ul>
<h2 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h2><ul>
<li>translate the output of the hypothesis function as follows: $$\begin{align*}& h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline& h_\theta(x) < 0.5 \rightarrow y = 0 \newline\end{align*}$$ </li>
<li>From these statements we can now say: $$\begin{align*}& \theta^T x \geq 0 \Rightarrow y = 1 \newline& \theta^T x < 0 \Rightarrow y = 0 \newline\end{align*}$$ 
</li>
</ul>
<h1 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h1><h2 id="Cost-function-for-one-variable-hypothesis"><a href="#Cost-function-for-one-variable-hypothesis" class="headerlink" title="Cost function for one variable hypothesis"></a>Cost function for one variable hypothesis</h2><ul>
<li>To let the cost function be convex for gradient descent, it should be like this: $$J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)})$$ 
</li>
</ul>
 $$Cost(h_\theta (x), y) =\begin{cases}-log(h_\theta (x)),  (y = 1) \\-log(1 - h_\theta (x)), (y = 0) \\\end{cases}$$ 
<ul>
<li>example $$\begin{align*}& \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*}$$ 
</li>
</ul>
<h2 id="Simplified-Cost-Function-and-Gradient-Descent"><a href="#Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="Simplified Cost Function and Gradient Descent"></a>Simplified Cost Function and Gradient Descent</h2><ul>
<li><p>compress our cost function’s two conditional cases into one case:</p>
 $$\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))$$ 
</li>
<li><p>entire cost function</p>
 $$J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$$ 
</li>
</ul>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><ul>
<li><p>the general form of gradient descent ，求偏导的得到$J(\theta)$的极值</p>
 $$\begin{align*}& Repeat \; \lbrace \newline & \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline & \rbrace\end{align*}$$ 
</li>
<li><p>using calculus </p>
 $$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum\limits_{i=1}^{m}[(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ 
</li>
<li><p>get</p>
 $$\begin{align*} & Repeat \; \lbrace \newline & \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline & \rbrace \end{align*}$$  
</li>
</ul>
<h2 id="Multiclass-Classification-One-vs-all"><a href="#Multiclass-Classification-One-vs-all" class="headerlink" title="Multiclass Classification: One-vs-all"></a>Multiclass Classification: One-vs-all</h2><ul>
<li>For more than 2 features of y, do logisitc regression for each feature separately</li>
<li>Train a logistic regression classifier $h_\theta(x)$ for each class￼ to predict the probability that ￼ ￼y = i￼ ￼. </li>
<li>To make a prediction on a new x, pick the class ￼that maximizes $ h_\theta (x) $<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170327/182954425.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<h1 id="Solving-the-Problem-of-Overfitting"><a href="#Solving-the-Problem-of-Overfitting" class="headerlink" title="Solving the Problem of Overfitting"></a>Solving the Problem of Overfitting</h1><h2 id="The-Problem-of-Overfitting"><a href="#The-Problem-of-Overfitting" class="headerlink" title="The Problem of Overfitting"></a>The Problem of Overfitting</h2><p><img src="http://cdn.mmmxcc.cn/blog/20170327/183239547.png" alt="mark"></p>
<h2 id="address-the-issue-of-overfitting"><a href="#address-the-issue-of-overfitting" class="headerlink" title="address the issue of overfitting"></a>address the issue of overfitting</h2><ul>
<li>Reduce the number of features:<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm (studied later in the course).</li>
</ul>
</li>
<li>Regularization:<ul>
<li>Keep all the features, but reduce the magnitude of parameters $θ_j$. </li>
<li>Regularization works well when we have a lot of slightly useful features.</li>
</ul>
</li>
</ul>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170327/184128038.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>in a single summation $$min_\theta\ \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2$$ 
</li>
</ul>
<p>The λ, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated.</p>
<h2 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h2><ul>
<li><p>Gradient Descent</p>
 $$\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}$$ 
</li>
<li><p>Normal Equation</p>
 $$\begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline& \text{where}\ \ L = \begin{bmatrix} 0 & & & & \newline & 1 & & & \newline & & 1 & & \newline & & & \ddots & \newline & & & & 1 \newline\end{bmatrix}\end{align*}$$ 
<ul>
<li>L is a matrix with 0 at the top left and 1’s down the diagonal, with 0’s everywhere else. It should have dimension (n+1)×(n+1)</li>
<li>Recall that if m ≤ n, then $X^TX$ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L $becomes invertible.</li>
</ul>
</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>我在这里整理一下上述两个方法，补全课程上的相关推导。</p>
<h2 id="Logistic-Regression-Model-1"><a href="#Logistic-Regression-Model-1" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h2><h3 id="h-theta-x-是假设函数"><a href="#h-theta-x-是假设函数" class="headerlink" title="$h_\theta(x)$是假设函数"></a>$h_\theta(x)$是假设函数</h3> $$h_\theta (x) = g ( \theta^T x ) = \dfrac{1}{1 + e^{- \theta^T x}} $$ 
<p>注意假设函数和真实数据之间的区别</p>
<h3 id="Cost-Function-1"><a href="#Cost-Function-1" class="headerlink" title="Cost Function"></a>Cost Function</h3> $$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large]$$ 
<p>回头看看上边的那个$h_\theta (x)$ ，cost function定义了训练集给出的结果 和 当前计算结果之间的差距。当然，该差距越小越好，那么需要求导一下。</p>
<h3 id="Gradient-Descent-1"><a href="#Gradient-Descent-1" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><ul>
<li>原始公式 $$\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta)$$ </li>
<li>求导计算 $$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum\limits_{i=1}^{m}[(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ </li>
<li>计算结果 $$\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} $$ 
</li>
</ul>
<p>这里推导一下$\frac{\partial}{\partial \theta_j} J(\theta)$：</p>
<ul>
<li><p>计算$h_\theta’(x)$导数</p>
 $$\begin{align*} &h_\theta'(x) = ( \frac1{1+e^{- \theta x}})'\newline &\ \ \ \ \ \ \ \  =   \frac{e^{- \theta x}x}{1+e^{- \theta x}}\newline &\ \ \ \ \ \ \ \  =   \frac{1+e^{- \theta x}-1}{(1+e^{- \theta x})^2}x\newline &\ \ \ \ \ \ \ \  =  \large[\frac{1}{1+e^{- \theta x}}-\frac{1}{(1+e^{- \theta x})^2}\large]x\newline &\ \ \ \ \ \ \ \  = h_\theta(x)(1-h_\theta(x))x  \end{align*}$$ 
</li>
<li><p>推导$\frac{\partial}{\partial \theta_j} J(\theta)$</p>
</li>
</ul>
 $$\begin{align*} &\frac{\partial}{\partial \theta_j} J(\theta) = \frac{\partial}{\partial \theta_j}   \frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}\ \log (h_\theta (x^{(i)})) - (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \  =   \frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}\ \frac1{h_\theta(x^{(i)})}h_\theta'(x^{(i)}) - (1 - y^{(i)}) \frac{-1}{1-h_\theta(x^{(i)})}h_\theta'(x^{(i)})\large] \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \  =   \frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}\ \frac1{h_\theta(x^{(i)})}h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}  \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ - (1 - y^{(i)}) \frac{-1}{1-h_\theta(x^{(i)})}h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}\large] \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \  =   \frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}(1-h_\theta(x^{(i)}) x^{(i)})+(1- y)h_\theta(x^{(i)}) x^{(i)})\large]  \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \  =   \frac{1}{m} \sum_{i=1}^m \large[ -x^{(i)}y^{(i)}+x^{(i)}y^{(i)}h_\theta(x^{(i)}) \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ +x^{(i)}h_\theta(x^{(i)}) - x^{(i)}y^{(i)}h_\theta(x^{(i)}) \large] \newline &\ \ \ \ \ \ \ \ \ \ \ \ \ \  = \frac{1}{m}\sum\limits_{i=1}^{m}[(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}] \end{align*}$$ 
<p>即：<br> $$\begin{align*} &\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum\limits_{i=1}^{m}[(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}] \end{align*}$$ </p>
<h2 id="Solving-the-Problem-of-Overfitting-1"><a href="#Solving-the-Problem-of-Overfitting-1" class="headerlink" title="Solving the Problem of Overfitting"></a>Solving the Problem of Overfitting</h2><p>其他地方都一样，稍作修改</p>
<h3 id="Cost-Function-2"><a href="#Cost-Function-2" class="headerlink" title="Cost Function"></a>Cost Function</h3> $$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$ 
<h3 id="Gradient-Descent-2"><a href="#Gradient-Descent-2" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3> $$\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}$$ 
<hr>
<p>以上</p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-03-28T07:49:36.625Z" itemprop="dateUpdated">2017年3月28日 15:49</time>
</span><br>


        转载请注明出处：<a href="/2017/03/27/Coursera-ML-4-Logistic-Regression/" target="_blank" rel="external">http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/</a>
    </div>
    <footer>
        <a href="http://feiyang.li">
            <img src="/img/logo.jpg" alt="Wing Lee">
            Wing Lee
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读书笔记/">读书笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&title=《Coursera ML(4)-Logistic Regression》 — 李飞阳&pic=http://feiyang.li/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&title=《Coursera ML(4)-Logistic Regression》 — 李飞阳&source=
本节笔记对应第三周Coursera课程  binary classification problem

" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Coursera ML(4)-Logistic Regression》 — 李飞阳&url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&via=http://feiyang.li" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/03/30/Coursera-ML-5-Programming-Exercise-2/index.html" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Coursera ML(5)-Logistic Regression and Regularization with Python</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/03/26/Coursera-ML-3-Multivariate-Linear-Regression-python/index.html" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Coursera ML(3)-Multivariate Linear Regression python实现</h4>
      </a>
    </div>
  
</nav>



    






<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2008929"></script>
<!-- UY END -->



</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        请我吃辣条好不好 ಠ౪ಠ
        <i class="icon icon-quote-right"></i>
    </h3>
    <ul class="reward-items">
        
        <li>
            <img src="/img/wechat.jpg" title="微信打赏二维码" alt="微信打赏二维码">
            <p>微信</p>
        </li>
        

        
        <li>
            <img src="/img/alipay.jpg" title="支付宝打赏二维码" alt="支付宝打赏二维码">
            <p>支付宝</p>
        </li>
        
    </ul>
</div>



</div>
 

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>





<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&title=《Coursera ML(4)-Logistic Regression》 — 李飞阳&pic=http://feiyang.li/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&title=《Coursera ML(4)-Logistic Regression》 — 李飞阳&source=
本节笔记对应第三周Coursera课程  binary classification problem

" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Coursera ML(4)-Logistic Regression》 — 李飞阳&url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/&via=http://feiyang.li" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://feiyang.li/2017/03/27/Coursera-ML-4-Logistic-Regression/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACs0lEQVR42u3ay4ojMQwF0P7/n+6BWQ0Mqdwr2f2AU6uQcqp8HLCFpI+P+Pr8e736/t+7r0a+etr/4z8fr48bFx4eHt5o6s+Pa1/wzH5+cvurN0uAh4eHd403276flym5+7xkCfWNBQ8PD+9beXn424bXSciOh4eH99t5eUJhP108PDy8n8nLt/g2IE5SBs/pj/wuHh4e3tfw2gLYT/h8sb6Hh4eHt6iqz7b1fHxyAGxmi4eHh3eDl7wsSVvMQu1Z01WeIsHDw8O7x5ulAzaJjLy01hbk6i4wPDw8vBEvT6rOilWbMLpd+qi+h4eHh7fgzWB5Eat9wizgfvk9Hh4e3gVeEry2yYW28D9jF0E2Hh4e3lHebD3a1EO7KO1M8PDw8L6Sl7yyHZls9/nW3z4HDw8P7yt5bfkqmUqe5tg3KES5ajw8PLxDvDY43pSskvFtM0FUBsPDw8M7xJtt9Js2rLYZqz0Yil4GPDw8vAVvs63nZf5NW8BwufHw8PBu7eQnug/aKH40si3L4eHh4d3gbcLfTfmqba6atRrg4eHh3eDlge9mTBTslo0Lwyw1Hh4e3iHerPCfHxhtB9SswFb0lOHh4eEd5bVFrFm6Ng+Uz9az8PDw8E7xziYI9it9OBGMh4eHd5S32fT3zVVJYJ0cIcWRgIeHh7fmba5TqYT8qJjdxcPDwzvLax+9CZHb8D0J1t/8b3h4eHgXePsi/ebFmyaDdm54eHh4p3if5TU7KjaphFn7V1HHw8PDwyt57eabf59PaJakGLZb4eHh4R3izYpSSYkr2eJnpbjit3h4eHjXeGeTBZui/uwEezk3PDw8vB/J27RSzYLp/Ml4eHh438vLXzabVn7k5K0GeHh4ePd4m2TELATPD4nVr/Dw8PAu8GYJgrMtBW3RK/lL8PDw8C7w/gCgjgMnewDUIwAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    
    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };



</script>

<script src="/js/main.min.js?v=1.4.11"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.4.11" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>





<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <footer class="footer">
    <div class="bottom">
        <p>
            <span><a href="http://www.beian.gov.cn/" target="_blank">陕ICP备17001447号</a></span>
            <span><a href="http://feiyang.li/sitemap.xml" target="_blank">网站地图</a></span>
        </p>
    </div>
</footer>

</body>
</html>
