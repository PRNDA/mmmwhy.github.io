<!DOCTYPE html>
<html>
<head>
    

    

    
<!-- Baidu Tongji -->
<script>var _hmt = _hmt || []</script>
<script async src="//hm.baidu.com/hm.js?d543f9aed343f96406eb3cabc2637f47"></script>
<!-- End Baidu Tongji -->



<!-- Baidu Push -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<!-- End Baidu Push -->


    <meta charset="utf-8">
    
    
    
    <title>Coursera ML(13)-SVM(Support Vector Machines) | 李飞阳 | PM、Coder、Data mining</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#0288D1">
    
    
    <meta name="keywords" content="Machine Learning,读书笔记">
    <meta name="description" content="本节笔记对应第七周课程和西瓜书第六章 支持向量机，主要讲解了支持向量机。">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera ML(13)-SVM(Support Vector Machines)">
<meta property="og:url" content="http://feiyang.li/2017/05/22/Coursera-ML-13/index.html">
<meta property="og:site_name" content="李飞阳">
<meta property="og:description" content="本节笔记对应第七周课程和西瓜书第六章 支持向量机，主要讲解了支持向量机。">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/142916098.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/143132931.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/145216127.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/121959778.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/150009680.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/151700413.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:image" content="http://cdn.mmmxcc.cn/blog/20170522/200058441.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
<meta property="og:updated_time" content="2017-05-22T12:32:31.116Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera ML(13)-SVM(Support Vector Machines)">
<meta name="twitter:description" content="本节笔记对应第七周课程和西瓜书第六章 支持向量机，主要讲解了支持向量机。">
<meta name="twitter:image" content="http://cdn.mmmxcc.cn/blog/20170522/142916098.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
">
    
        <link rel="alternate" type="application/atom+xml" title="李飞阳" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.6.7">
    <script>window.lazyScripts=[]</script>
</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/logo.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Wing Lee</h5>
          <a href="mailto:mmmwhy@qq.com" title="mmmwhy@qq.com" class="mail">mmmwhy@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/Coursera-ML"  >
                <i class="icon icon-lg icon-paperclip"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/mmmwhy" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/2016/01/18/resume/"  >
                <i class="icon icon-lg icon-cog"></i>
                关于
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/link.html"  >
                <i class="icon icon-lg icon-link"></i>
                膜拜大佬
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://ss.feiyang.li/" target="_blank" >
                <i class="icon icon-lg icon-rocket"></i>
                肥羊SS
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Coursera ML(13)-SVM(Support Vector Machines)</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Coursera ML(13)-SVM(Support Vector Machines)</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-05-22T12:25:02.000Z" itemprop="datePublished" class="page-time">
  2017-05-22
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Coursera-ML/">Coursera ML</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#SVM介绍"><span class="post-toc-number">1.</span> <span class="post-toc-text">SVM介绍</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数学上的定义"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">数学上的定义</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#逻辑回归模型"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">逻辑回归模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#将模型进行抽象"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">将模型进行抽象</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#直观化的定义"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">直观化的定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#核函数－核函数"><span class="post-toc-number">2.</span> <span class="post-toc-text">核函数－核函数</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#为什么要使用核函数"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">为什么要使用核函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#详细介绍"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">详细介绍</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#软间隔与正则化"><span class="post-toc-number">3.</span> <span class="post-toc-text">软间隔与正则化</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#回归问题－支持向量回归"><span class="post-toc-number">4.</span> <span class="post-toc-text">回归问题－支持向量回归</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#核-nuclear-方法"><span class="post-toc-number">5.</span> <span class="post-toc-text">核(nuclear)方法</span></a></li></ol>
        </nav>
    </aside>
    
<article id="post-Coursera-ML-13"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Coursera ML(13)-SVM(Support Vector Machines)</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-05-22 20:25:02" datetime="2017-05-22T12:25:02.000Z"  itemprop="datePublished">2017-05-22</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Coursera-ML/">Coursera ML</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><blockquote>
<p>本节笔记对应<a href="https://www.coursera.org/learn/machine-learning/home/week/7" target="_blank" rel="external">第七周课程</a>和西瓜书<strong>第六章 支持向量机</strong>，主要讲解了支持向量机。</p>
</blockquote>
<hr>
<a id="more"></a>
<h1 id="SVM介绍"><a href="#SVM介绍" class="headerlink" title="SVM介绍"></a>SVM介绍</h1><p>这一部分Coursera上的内容我<strong>并没有听懂</strong>，一头雾水。</p>
<h2 id="数学上的定义"><a href="#数学上的定义" class="headerlink" title="数学上的定义"></a>数学上的定义</h2><h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><p>从最简单的逻辑回国模型说起<br> $$\begin{align*}& h_\theta (x) = g ( \theta^T x ) \newline \newline& z = \theta^T x \newline& g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}$$<br>简单点，也就是<br>$$h_\theta (x) = \dfrac{1}{1 + e^{-\theta^T x}} $$<br>其图像为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/142916098.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>我们期望的的是底下那两行，而实际给我们的这个sigmoid函数，好像并没有那么完美，这是我们进行少许的处理。</p>
<h3 id="将模型进行抽象"><a href="#将模型进行抽象" class="headerlink" title="将模型进行抽象"></a>将模型进行抽象</h3><p>原本的 $h_\theta (x) = log  \dfrac{1}{1 + e^{-\theta^T x}}$ 或者 $h_\theta (x) = log(1-\dfrac{1}{1 + e^{-\theta^T x}})$ 的图像应该是底下的黑线。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/143132931.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>这里，我们定义一个新的新的符号<strong>cost</strong>，就是上边那个紫色的两条直线组成的东西。<br><strong>为什幺这么做</strong>，食屎啦，我怎么知道为什么这么做，先留着这个问题。</p>
<p>改造一番之后，我们得到了<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/145216127.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>这个就是支持向量机的数学定义，去他妈的，什么鬼东西，干什么了嘛，SVM就被定义出来了。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/121959778.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<h2 id="直观化的定义"><a href="#直观化的定义" class="headerlink" title="直观化的定义"></a>直观化的定义</h2><p>给定这样一个样本集$D={X_1,y_1},{X_2,y_2},{X_3,y_3}……{X_m,y_m},y_i \in(-1,+1)$<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/150009680.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>我们可以找到很多条直线分割这两个区域<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/151700413.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="mark" title="">
                </div>
                <div class="image-caption">mark</div>
            </figure><br>那么我们应该努力去找到哪一个呢？直观上看，那个正中间的黑线看起来很不错。</p>
<p><strong>因为该划分超平面对训练样本的局部扰动容忍度最好。</strong></p>
<ul>
<li>划分超平面可以定义为如下线性方程组：<br>$$w^Tx+b=0$$<br>其中$w=(w_1;w_2…;w_d;)$为法向量，决定平面的方向，$b$是位移量。<br>将其标记为：$(w,b)$，样本中任一点到超平面$(w,b)$的距离可以表示为：<br>$$r=    \dfrac{|w^T+b|}{||w||}$$</li>
<li>假定成功分类，那： $$ \left\{
\begin{aligned}
w^T+b \geq1,   y_i=+1\\
w^T+b \leq-1,   y_i=-1
\end{aligned}
\right.
$$ </li>
<li><p>两个异类支持向量到超平面的距离之和(间隔)为：<br>$$\gamma= \dfrac 2{||w||}$$<br>那么如果我们可以找到最大的间隔，是不是就可以使这个分割线处于最中间的位置。</p>
</li>
<li><p>使距离最小化<br>我们换一个表达方式</p>
</li>
</ul>
<p>$$\begin{aligned}<br>\min \limits_{w,b} \dfrac   12||w||^2 \<br>s.t. y_i(w^Tx+b)  \geq 1<br>\end{aligned}<br>$$<br>这就是支持向量机的基本型</p>
<p>#万能的拉格朗日－对偶问题</p>
<p>又有周boss的“简单”的数学推导了：这一次是和拉格朗日一起。基本SVM的优化目标本身是一个凸二次规划问题，可以利用优化计算包求解，但是据说拉格朗日有更高效的解法：<br>对上一节最后式子使用拉格朗日乘子法就可以得到它的对偶问题 （dual problem），对偶问题出现在线性规划中，每一个求极小的线性规划问题都有一个求极大的线性规划问题互称对偶问题，解决了一个就对应解决了其对偶问题。具体到这里，对式子的每条约束添加非负拉格朗日乘子$\alpha_i$，拉格朗日函数是<br> $$L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^2+\sum_{i=1}^m{\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))}$$<br>对其去$\boldsymbol{w}$和$b$的偏导为0可得到对偶问题：<br> $$max_\boldsymbol{\alpha}\ \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j$$$$s.t.\ \sum_{i=1}^m{\alpha_iy_i}=0,\ \alpha_i\geq0,\ i=1,2,…,m$$<br>这又是一个二次规划问题，但通用解法规模正比于样本数，在大规模数据上开销很大，可以考虑其它高效算法如SMO (Sequential Minimal Optimization)。SMO的简单描述是其不断重复以下两个步骤直至收敛：</p>
<p>选取一对需更新的变量$\alpha_i$和$\alpha_j$<br>固定$\alpha_i$和$\alpha_j$以外的参数，求解对偶问题更新后的$\alpha_i$和$\alpha_j$。</p>
<p>SMO算法在一时间也不能得到完整的理解，希望能作为学习二次规划问题的一个复杂例子在之后研究。</p>
<h1 id="核函数－核函数"><a href="#核函数－核函数" class="headerlink" title="核函数－核函数"></a>核函数－核函数</h1><h2 id="为什么要使用核函数"><a href="#为什么要使用核函数" class="headerlink" title="为什么要使用核函数"></a>为什么要使用核函数</h2><p>原始样本空间也许并不存在一个可以正确划分两类样本的超平面，一个比较好的解决办法，试讲当前原始空间映射到一个高维空间去。根据<strong>某些大佬的研究</strong>，如果原始样本空间是有限的，那么总可以找到一个高维特性空间使样本可分。</p>
<p>其实这个问题在西瓜书上说的非常清楚，但是笔记却无法做，主要是相关的图我找不出来，好气哦。</p>
<h2 id="详细介绍"><a href="#详细介绍" class="headerlink" title="详细介绍"></a>详细介绍</h2><p>取映射函数$\phi$，特征空间划分超平面模型表示为：<br>$$f(x)=\boldsymbol{w}^T\phi(x)+b$$<br>类似的有问题变为了：<br> $$min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2$$$$s.t.\ y_i(\boldsymbol{w}^T\phi(\boldsymbol{x}_i)+b)\geq1,\ i=1,2,…,m$$<br>其对偶问题是：<br> $$max_\boldsymbol{\alpha}\ \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}_i^T)\phi(\boldsymbol{x}_j)$$$$s.t.\ \sum_{i=1}^m{\alpha_iy_i}=0,\ \alpha_i\geq0,\ i=1,2,…,m$$ </p>
<p>这中间涉及了$\phi(\boldsymbol{x}_i^T)\phi(\boldsymbol{x}_j)$，是样本映射到高维空间后的内积，由于维数可能很高甚至无穷，其计算很困难。此时我们就需要找一个函数，使得：<br>$$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=\phi(\boldsymbol{x}_i^T)\phi(\boldsymbol{x}_j)$$<br>称作核函数 (kernal function)。<br>有定理表明：只要一个对称函数对应的核矩阵半正定，它就能作为核函数使用。几种常用的核函数有：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th style="text-align:center">表达式</th>
<th style="text-align:left">参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\boldsymbol{x}_i^T\boldsymbol{x}_j$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td>多项式核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i^T\boldsymbol{x}_j)^d$</td>
<td style="text-align:left">$d \geq 1$为多项式的次数</td>
</tr>
<tr>
<td>高斯核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{∥\boldsymbol{x}_i-\boldsymbol{x}_j∥^2}{2\sigma^2})$</td>
<td style="text-align:left">$\sigma \geq 0$为高斯核的带宽</td>
</tr>
<tr>
<td>拉普拉斯核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{∥\boldsymbol{x}_i-\boldsymbol{x}_j∥}{\sigma})$</td>
<td style="text-align:left">$\sigma \geq 0$</td>
</tr>
<tr>
<td>Sigmoid核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\tanh(\beta\boldsymbol{x}_i^T\boldsymbol{x}_j+\theta)$</td>
<td style="text-align:left">$\tanh$为双曲正切函数，$\beta&gt;0,\theta&lt;0$</td>
</tr>
</tbody>
</table>
<p>多个核函数的线性组合，两个核函数的直积均为核函数，以及若$\kappa_1$为核函数，对于任意函数$g(\boldsymbol{x})$有：$\kappa(\boldsymbol{x},\boldsymbol{z})=g(\boldsymbol{x})\kappa_1(\boldsymbol{x},\boldsymbol{z})g(\boldsymbol{z})$也是核函数<br>如何选择核函数将样本映射到真正显示其分布规律的高维空间非常重要。</p>
<h1 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h1><p>实际上很多问题并不能简单地归结为线性可分线性不可分，一些问题的样本空间看来是线性可分的，但总有几个样本跑到了敌对阵营，就为此强行升高维度显然得不偿失。于是引入了软间隔 (soft margin)：允许有若干个样本被线性空间划分错误。此时的优化目标变为：<br> $$min_{\boldsymbol{w},b}\ \frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^m\ell_{0/1}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)-1)$$<br>其中$C$是一个常数，可以理解问允许多少个样本被分错，而$\ell_{0/1}$是0/1损失函数，当样本被分错时，它的取值为1，否则为0。这个函数直观，简单，然而非凸，非连续，常用一些替代函数：</p>
<p>hinge损失：$\ell<em>{hinge}(z)=\max(0,1,1-z)$<br>指数损失 (exponential loss)：$\ell</em>{exp}(z)=\exp(-z)$<br>对率损失 (logistic loss)：$\ell_{log}(z)=\log(1+\exp(-z))$</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://cdn.mmmxcc.cn/blog/20170522/200058441.png?imageView2/0/format/webp/interlace/1/q/75|watermark/2/text/ZmVpeWFuZy5saQ==/font/Y29taWMgc2FucyBtcw==/fontsize/500/fill/IzAzQTlGNA==/dissolve/35/gravity/SouthEast/dx/10/dy/10|imageslim
" alt="三种常见的替代损失函数对应图像为" title="">
                </div>
                <div class="image-caption">三种常见的替代损失函数对应图像为</div>
            </figure>
<ul>
<li>红色为：hinge损失</li>
<li>蓝色为：指数损失</li>
<li>黑色为：对率损失<br>这个地方跟最开始的时候提到的图像好像，只不过Andrew直接就把结果说出来了，并没有解释为什么罢了。</li>
</ul>
<p>代入优化目标后同样可以找到它们的对偶问题进行求解。实际上此时的优化目标可以看作两部分：第一项描述划分超平面的间隔大小，第二项描述数据集上的误差。对于误差还有更一般的形式： $$min_{f}\ \Omega(f)+C\sum_{i=1}^m\ell(f(\boldsymbol{x}_i),y_i)$$ 其中$\Omega(f)$称为结构风险，后一项称为经验风险，$C$用于对二者进行折中。结构风险方便引入领域知识和用户意图：用户希望得到何种性质的模型，同时它也有助于削减假设空间。这个式子称为正则化 (regularization)。</p>
<h1 id="回归问题－支持向量回归"><a href="#回归问题－支持向量回归" class="headerlink" title="回归问题－支持向量回归"></a>回归问题－支持向量回归</h1><p>普通的回归问题计算损失为函数预测值与真实值的差，而使用支持向量回归 (Support Vector Regression, SVR)，我们容忍$\epsilon$的误差，那就相当于在预测函数两端建立了一个宽为$2\epsilon$的隔离带，在此间隔带种的样本被默认为预测正确，而在此间隔带外的样本计算它的真实值与预测函数得到值的差作为损失，就此SVR问题可形式化为：<br> $$min_{\boldsymbol{w},b}\ \frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^m\ell_{\epsilon}(f(\boldsymbol{x}_i)-y_i)$$ </p>
<p>其中$C&gt;0$是一个常数,$l_{0/1}$是”0/1”损失函数<br> $$\begin{equation}l_{0/1}=\left\{\begin{array}{ll}
1,\,\text{if}\quad z<0\\ 0,otherwise="" \end{array}\right.\end{equation}$$="" <="" p="">
<p>常见的损失函数有hinge损失,指数损失,对率损失(见P130).</p>
<p>采用hinge损失,引入”松弛变量”(slack variables),上式可重写为<br> $$\begin{equation}\begin{split}
\min_{\mathbf w,b,\xi}&\frac{1}{2}\|\mathbf w\|^2+C\sum_{i=1}^m\xi_i\\
\text{s.t.}&\,y_i(\mathbf{w^Tx_i}+b)\ge1-\xi_i\\
&\xi_i\ge0,\,i=1,2,\cdots,m.
\end{split}\end{equation}$$ </p>
<p>对偶问题推导以及解稀疏性证明见P132.</p>
<p>支持向量引申到回归问题上即可得到支持向量回归(Support Vector Regression, SVR).SVR假设我们能容忍$f(\mathbf x)$与$y$之间最多有$\epsilon$的偏差,即仅当$f(\mathbf x)$与$y$之间的差别绝对值大于$\epsilon$时才计算损失.SVR问题可化为<br> $$\begin{equation}\begin{split}
\min_{\mathbf w,b}\frac{1}{2}\|\mathbf w\|^2+C\sum_{i=1}^ml_\epsilon(f(\mathbf x_i)-y_i)
\end{split}\end{equation}$$ </p>
<p>其中 $ C $为正则化常数,$l_\epsilon$为$\epsilon$-不敏感损失函数</p>
 $$\begin{equation}l_\epsilon=\left\{\begin{array}{ll}
0,\quad\text{if}|z|\le\epsilon\\
|z|-\epsilon,otherwise
\end{array}\right.\end{equation}$$ 
<p>对偶问题推导以及KKT条件证明见P135-137.</p>
<h1 id="核-nuclear-方法"><a href="#核-nuclear-方法" class="headerlink" title="核(nuclear)方法"></a>核(nuclear)方法</h1><p>给定训练样本,不考虑偏移项$b$,则无论SVM还是SVR,学得的模型总能表示成核函数$\kappa(\mathbf{x_i,x_j})$的线性组合.因此有更一般的”表示定理”(representer theorem).P137。</p>
<p>由此人们发展了一系列基于核函数的学习方法，统称为核方法 (kernel methods)。最常见的就是引入核函数来将线性学习器拓展为非线性学习器，从而得到核线性判别分析 (Kernelized Linear Discriminant Analysis)。</p>
</0\\></p>
        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-05-22T12:32:31.116Z" itemprop="dateUpdated">2017-05-22 20:32:31</time>
</span><br>


        
        可以转载，请注明出处：<a href="/2017/05/22/Coursera-ML-13/" target="_blank" rel="external">http://feiyang.li/2017/05/22/Coursera-ML-13/</a>
        
    </div>
    <footer>
        <a href="http://feiyang.li">
            <img src="/img/logo.jpg" alt="Wing Lee">
            Wing Lee
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读书笔记/">读书笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://feiyang.li/2017/05/22/Coursera-ML-13/&title=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&pic=http://feiyang.li/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://feiyang.li/2017/05/22/Coursera-ML-13/&title=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&source=
本节笔记对应第七周课程和西瓜书第六章 支持向量机，主要讲解了支持向量机。

" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://feiyang.li/2017/05/22/Coursera-ML-13/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&url=http://feiyang.li/2017/05/22/Coursera-ML-13/&via=http://feiyang.li" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://feiyang.li/2017/05/22/Coursera-ML-13/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/05/24/algorithm1/index.html" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">算法实验(一)</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/05/21/django3/index.html" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Django 笔记之三 Django 模板</h4>
      </a>
    </div>
  
</nav>



    










    <div id="cloud-tie-wrapper" class="comments cloud-tie-wrapper"></div>
    <script>
    var cloudTieConfig = {
        url: 'http://feiyang.li/2017/05/22/Coursera-ML-13/index.html',
        sourceId: 'Coursera-ML-13',
        productKey: '9d56d2fd321c48c7a0b079b76f0795cf',
        target: 'cloud-tie-wrapper',
        pcFiles: [
            '/css/cloudTie/pc.css?v=1.6.7',
            '/js/cloudTie/pc.min.js?v=1.6.7',
        ],
        mobileFiles: [
            '/css/cloudTie/mobile.css?v=1.6.7',
            '/js/cloudTie/mobile.min.js?v=1.6.7'
        ]
    };
    </script>
    <script src="/js/cloudTie/loader.min.js?v=1.6.7"></script>







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <ul class="reward-items">
        
        <li>
            <img src="/img/wechat.jpg" title="微信打赏二维码" alt="微信打赏二维码">
            <p>微信</p>
        </li>
        

        
        <li>
            <img src="/img/alipay.jpg" title="支付宝打赏二维码" alt="支付宝打赏二维码">
            <p>支付宝</p>
        </li>
        
    </ul>
</div>



</div>

    </main>
    <footer class="footer">
    <div class="bottom">
        <p>
            <span><a href="http://www.beian.gov.cn/" target="_blank">陕ICP备17001447号</a></span>
            <span><a href="https://www.wishosting.com/" target="_blank">support by wishosting</a></span>
        </p>
    </div>
</footer>

    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://feiyang.li/2017/05/22/Coursera-ML-13/&title=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&pic=http://feiyang.li/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://feiyang.li/2017/05/22/Coursera-ML-13/&title=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&source=
本节笔记对应第七周课程和西瓜书第六章 支持向量机，主要讲解了支持向量机。

" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://feiyang.li/2017/05/22/Coursera-ML-13/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Coursera ML(13)-SVM(Support Vector Machines)》 — 李飞阳&url=http://feiyang.li/2017/05/22/Coursera-ML-13/&via=http://feiyang.li" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://feiyang.li/2017/05/22/Coursera-ML-13/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKUlEQVR42u3aQW7DMAwEwPz/0+m5COLsUmkBS+NT0bq2xgdC1PLxiK/ny/X619efr+95fdq753/twsDAuC3jeXnl9+RvucYP34uBgXEAIymR1/ckRTP/WO17MTAwMGabv2Rb+a3Ph4GBgXH9+raA5kUWAwMDo21i86XPmtil5hkDA2NrRn5Y9v8//0m+gYGBcSvGs7za7ePsafWqMDAwtmbkBW62IUvGMq7vKdaDgYGxKWPWLs7KZRshFJ8PAwPjAEZya4JZDwZy8K/fYGBgHMBoRyVawKyI580zBgbG3ow2jEwY68MWwyEzDAyMTRn5uMPKmEUybLESJ2BgYOzNaDd/0RatHAJrm9u3HwIDA2NTRhsx5q1sfrjfHrF9iDAxMDC2Y8zGvFYWPXtXm1RiYGDsxGiDgXa0qz35azemGBgYJzDyCas2Ymyj0KVhCwwMjK0Z+ZFW3ljmjWtCij4TBgbGAYxZAJkj22Xlx3MYGBgnMFY6wZVi3calS20tBgbGFox6AKvcDs4iybb4YmBgnMBIFjoLGNrf5AdtGBgYpzHyXWQdKJahaf6Eos/GwMC4LeNZXsmmLX9CezxX5K4YGBgbMdpCOWt327mPFoyBgXECoy2R6/HASiCx9KUxMDBuzpgVynzwq40Thr04BgYGxpc6x1kg8eF/MTAwMIJt3/ro2MoRHgYGxgmMPAxoy+XsYC4v4hgYGCcwZq3jLOBsi+z10jEwMA5g/AC4P9sZPMGMFAAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };



</script>

<script src="/js/main.min.js?v=1.6.7"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.6.7" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
